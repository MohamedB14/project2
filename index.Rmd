---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Mohamed Bettayeb, mb58422

### Introduction 

My dataset consists of data collected from incidents of heart failures at hospitals. The variables included in this dataset are age, anaemia, creatinine_phosphokinase, diabetes, ejection_fraction, high_blood_pressure, platelets, serum_creatinine, serum_sodium, sex, smoking, time, and DEATH_EVENT. I found this data on kaggle.com. My variables are measuring certian statuses and data collected by the hospitals of the patients that had an incident of heart failure. Age marks the age of the patient. Anaemia marks whether or not the patient had a deficiency in red blood cells. Creatinine_phosphokinase, serum_creatinine, aand serum_sodium are variables that measure how much of their respective chemicals are in the patient's bloodstream. Ejection fraction measures the amount of blood leaving the heart. Diabetes marks whether or not the patient has diabetes. High_blood_pressure marks whether or not the patient has high blood pressure. Platelets measures the platelet count of the patient. Sex marks the sex of the patient. Smoking marks whether or not the patient is a smoker. Time measures the follow-up period. DEATH_EVENT marks whether or not this heart failure was fatal. I found this data particularly interesting as it has a lot to do with a particular field that I am interested in pursuing a career in one day; cardiology. There are approximately 299 observations of all 13 variables. 

```{R}
library(tidyverse)
library(dplyr)
library(readr)
library(gt)
# read your datasets in here, e.g., with read_csv()
heart_failure <- read_csv("heart_failure_clinical_records_dataset.csv")
# if your dataset needs tidying, do so here

# any other code here
```

### Cluster Analysis

```{R}
library(cluster)
library(ggplot2)
library(GGally)
pam_data <-heart_failure %>% select(age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time)
sil_width <- vector()
for (i in 2:10){
  pam_fit <- pam(pam_data, k = i)
  sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot() + geom_line(aes(x=1:10, y=sil_width)) + scale_x_continuous(name="k", breaks=1:10)
# 9 is the ideal k
pam1 <- heart_failure %>% select(age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time) %>% pam(k=9)
pamclust <- heart_failure %>% select(age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time) %>% mutate(cluster=as.factor(pam1$clustering))
ggpairs(pamclust, columns = 1:7,aes(color=cluster))
```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
h1 <- heart_failure %>% select(age, creatinine_phosphokinase, ejection_fraction, platelets, serum_creatinine, serum_sodium, time)
heartnums <- h1 %>% select_if(is.numeric) %>% scale
heartpca <- princomp(heartnums)
names(heartpca)
eigval <- heartpca$sdev^2
round(cumsum(eigval)/sum(eigval), 2)
heartdf <- data.frame(PC1 = heartpca$scores[,1], PC2 = heartpca$scores[,2])
ggplot(heartdf, aes(PC1, PC2)) + geom_point()
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
# linear classifier code here
fit <- glm(anaemia ~ age + creatinine_phosphokinase + ejection_fraction + platelets + serum_creatinine + serum_sodium + time, data = heart_failure, family= "binomial")
score <- predict(fit, type="response")
class_diag(score, heart_failure$anaemia, positive = 1)
table(predict=as.numeric(score >.5),truth=heart_failure$anaemia)%>%addmargins
```

```{R}
# cross-validation of linear classifier here
set.seed(1234)
k=10 #choose number of folds
heartdata<-heart_failure[sample(nrow(heart_failure)),] #randomly order rows
folds<-cut(seq(1:nrow(heart_failure)),breaks=k,labels=F) #create 10 folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-heartdata[folds!=i,] 
  test<-heartdata[folds==i,]
  truth<-test$anaemia
  ## Train model on training set
  fit<-glm(anaemia~.,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  ## Test model on test set (save all k results)
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
knn_fit <- knn3(factor(anaemia==1,levels=c("TRUE","FALSE")) ~., data=heart_failure, k=5)
y_hat_knn <- predict(knn_fit,heart_failure)
class_diag(y_hat_knn[,1],heart_failure$anaemia, positive=1)
table(predicted=as.numeric(y_hat_knn[,1] >.5),actual=heart_failure$anaemia)%>%addmargins

```

```{R}
# cross-validation of np classifier here
set.seed(1234)
k=5 #choose number of folds
cvdata<-heart_failure %>% sample_frac() #randomly order rows
folds<-cut(seq(1:nrow(heart_failure)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  cvtrain<-cvdata[folds!=i,]
  cvtest<-cvdata[folds==i,]
  ## Fit linear regression model to training set
  cvfit<-knnreg(anaemia~.,data=cvtrain)
  ## Get predictions/y-hats on test set (fold i)
  cvyhat<-predict(cvfit,newdata=cvtest)
  ## Compute prediction error  (MSE) for fold i
  cvdiags<-mean((cvtest$anaemia-cvyhat)^2) 
}
mean(cvdiags) ## get average MSE across all folds (much higher error)!

```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
lrfit<-lm(age~.,data=heart_failure) #predict mpg from all other variables
lryhat<-predict(lrfit) #predicted mpg
mean((heart_failure$age-lryhat)^2)
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 #choose number of folds
lcvdata<-heart_failure[sample(nrow(heart_failure)),] #randomly order rows
lcvfolds<-cut(seq(1:nrow(heart_failure)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  lcvtrain<-lcvdata[lcvfolds!=i,]
  lcvtest<-lcvdata[lcvfolds==i,]
  ## Fit linear regression model to training set
  lcvfit<-lm(age~.,data=lcvtrain)
  ## Get predictions/y-hats on test set (fold i)
  lcvyhat<-predict(fit,newdata=lcvtest)
  ## Compute prediction error  (MSE) for fold i
  lcvdiags<-mean((test$age-lcvyhat)^2) 
}
mean(lcvdiags) ## get average MSE across all folds (much higher error)!
```

Discussion

### Python 

```{R}
library(reticulate)

math <- 59
```

```{python}
# python code here
math = 58
print(r.math + math)
```

```{R}
sum <- sum(math, py$math)
sum
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




